{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "U1aTwgHGjNZ_",
        "oMmaeZNzjRMq",
        "tba-pc32jUpg",
        "AD04R5XfjY5q",
        "rlkUTxTZjbaE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Library"
      ],
      "metadata": {
        "id": "U1aTwgHGjNZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai pydantic python-dotenv pandas openai langchain_community pypdf ddgs faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "geTFT8htn-Ft",
        "outputId": "bb7cfac2-7a7c-48c5-83f4-8a5b96011054"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting ddgs\n",
            "  Downloading ddgs-9.5.5-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from ddgs)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting lxml>=6.0.0 (from ddgs)\n",
            "  Downloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ddgs-9.5.5-py3-none-any.whl (37 kB)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, primp, mypy-extensions, marshmallow, lxml, faiss-cpu, typing-inspect, ddgs, dataclasses-json, langchain-core, langchain-openai, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.75\n",
            "    Uninstalling langchain-core-0.3.75:\n",
            "      Successfully uninstalled langchain-core-0.3.75\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 ddgs-9.5.5 faiss-cpu-1.12.0 langchain-core-0.3.76 langchain-openai-0.3.33 langchain_community-0.3.29 lxml-6.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 primp-0.15.0 pypdf-6.0.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rbO3WN3Nncxx"
      },
      "outputs": [],
      "source": [
        "import os, faiss, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field\n",
        "from dotenv import load_dotenv\n",
        "from ddgs import DDGS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# LangChain Core\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# LangChain Agents & Tools\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.tools import tool\n",
        "\n",
        "# LangChain OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "response = llm([\n",
        "                SystemMessage(content=\"You are an Assistant to help find a job\"),\n",
        "                HumanMessage(content=\"Halo I'm Dastin\")\n",
        "                ])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "_IIJfAvcoGat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Tools"
      ],
      "metadata": {
        "id": "oMmaeZNzjRMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Tools\n",
        "def build_cv_parser(payChain):\n",
        "    \"\"\"\n",
        "    Builds a LangChain tool for parsing raw CV text into a structured format.\n",
        "    \"\"\"\n",
        "    @tool(\"cv_parser\", return_direct=False)\n",
        "    def cv_parser(tool_input: str):\n",
        "        \"\"\"\n",
        "        Ingest a raw CV text and parse it into a structured, machine-readable format (e.g., a JSON object or a Python dictionary).\n",
        "        The tool should handle basic data extraction for sections like experience, skills, education, and projects.\n",
        "\n",
        "        Args:\n",
        "            tool_input (str): The raw text content of the CV.\n",
        "\n",
        "        Returns:\n",
        "            str: A JSON string representing the structured CV data.\n",
        "        \"\"\"\n",
        "        messages = [\n",
        "            SystemMessage(content=(f\"\"\"\n",
        "            You are a skilled CV parser. Your task is to extract information from the provided raw CV text\n",
        "            and structure it into a JSON object.\n",
        "            The JSON object should include the following keys, if the information is available:\n",
        "            - \"experience\": A list of dictionaries, each representing a job experience with keys like \"title\", \"company\", \"dates\", \"description\".\n",
        "            - \"skills\": A list of strings representing key skills.\n",
        "            - \"education\": A list of dictionaries, each representing an educational background with keys like \"degree\", \"institution\", \"dates\", \"details\".\n",
        "            - \"projects\": A list of dictionaries, each representing a project with keys like \"name\", \"description\", \"technologies\".\n",
        "            - \"contact_info\": A dictionary with keys like \"email\", \"phone\", \"linkedin\", \"github\", \"location\".\n",
        "            - \"summary\" or \"objective\": A string containing the summary or objective statement.\n",
        "            - \"name\": The candidate's name.\n",
        "\n",
        "            If a section is not found in the CV, the corresponding key should be an empty list or an empty string/dictionary as appropriate.\n",
        "            Ensure the output is a valid JSON string.\n",
        "            \"\"\"\n",
        "            )),\n",
        "            HumanMessage(content=(f\"\"\"\n",
        "            Parse the following CV text into a JSON object:\n",
        "            {payChain.payload['curriculum_vitae']}\n",
        "            \"\"\"\n",
        "            ))\n",
        "        ]\n",
        "        response = llm(messages).content\n",
        "        payChain.payload[\"cv_data\"] = json.loads(response.replace(\"```json\", '').replace(\"`\",\"\"))\n",
        "\n",
        "        return {\n",
        "            \"structured_cv_data\": payChain.payload[\"cv_data\"][\"experience\"],\n",
        "        }\n",
        "    return cv_parser\n",
        "\n",
        "# Define Tools\n",
        "def build_skill_analyst(payChain):\n",
        "    \"\"\"\n",
        "    Builds a LangChain tool for analyzing structured CV data to infer skills.\n",
        "    \"\"\"\n",
        "    @tool(\"skill_analyst\", return_direct=False)\n",
        "    def skill_analyst(tool_input: str):\n",
        "        \"\"\"\n",
        "        Analyze the structured CV data to infer a deeper understanding of the candidate's\n",
        "        capabilities. This tool must look for implicit skills (e.g., a project on a specific framework\n",
        "        implies knowledge of related concepts) and transferable skills.\n",
        "\n",
        "        Args:\n",
        "            tool_input (str): A JSON string representing the structured CV data\n",
        "                              (output from the cv_parser tool).\n",
        "\n",
        "        Returns:\n",
        "            str: A JSON string or formatted text summarizing the inferred skills,\n",
        "                 including implicit and transferable skills.\n",
        "        \"\"\"\n",
        "        messages = [\n",
        "            SystemMessage(content=(f\"\"\"\n",
        "            You are a skill analysis expert. Your task is to analyze the provided structured CV data\n",
        "            and identify both explicit and implicit skills. Look for technologies mentioned in projects,\n",
        "            responsibilities in experience sections that imply certain abilities (e.g., leadership, communication, problem-solving),\n",
        "            and educational details that suggest foundational knowledge.\n",
        "\n",
        "            Present the inferred skills in a clear and organized format. You can categorize them\n",
        "            (e.g., Technical Skills, Soft Skills, Domain Knowledge) or simply list them with brief explanations\n",
        "            of why they were inferred. Highlight any transferable skills you identify.\n",
        "            \"\"\"\n",
        "            )),\n",
        "            HumanMessage(content=(f\"\"\"\n",
        "            Analyze the following structured CV data and infer the candidate's skills and provide A detailed analysis of explicit and implicit skills:\n",
        "            {payChain.payload[\"cv_data\"]}\n",
        "            \"\"\"\n",
        "            ))\n",
        "        ]\n",
        "        response = llm(messages).content\n",
        "        print(f\"Detailed Analysis: {response}\")\n",
        "        # payChain.payload[\"detailed_analysis\"] = json.loads(response.replace(\"```json\", '').replace(\"`\",\"\"))\n",
        "        payChain.payload[\"detailed_analysis\"] = response\n",
        "\n",
        "\n",
        "        return {\n",
        "            payChain.payload[\"detailed_analysis\"][:300]\n",
        "        }\n",
        "    return skill_analyst\n",
        "\n",
        "# Define Tools\n",
        "def build_market_searcher(payChain):\n",
        "    \"\"\"\n",
        "    Builds a LangChain tool for searching current job listings and industry trends.\n",
        "    \"\"\"\n",
        "    @tool(\"market_searcher\", return_direct=False)\n",
        "    def market_searcher(tool_input: str):\n",
        "        \"\"\"\n",
        "        Use a search tool to query current job listings and industry trends for a specified\n",
        "        technical role (e.g., \"Senior AI Engineer requirements\"). This tool should gather and\n",
        "        summarize in-demand skills and technologies based on the search results.\n",
        "\n",
        "        Args:\n",
        "            tool_input (str): A string specifying the technical role and search query\n",
        "                              (e.g., \"Senior AI Engineer requirements\").\n",
        "\n",
        "        Returns:\n",
        "            str: A summary of in-demand skills and technologies based on the search results.\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=(f\"\"\"You are a market research assistant. Your task is to create a list of Google search queries relevant to understanding the market, trends, and requirements of a specific job role.\n",
        "            The queries should cover industry trends, required skills, salary insights, challenges, and opportunities.\n",
        "\n",
        "            Return the answer only in JSON format:\n",
        "            {{\"queries\":[\"question 1\",\"question 2\",\"question 3\", ...]}}\n",
        "            \"\"\")),\n",
        "            HumanMessage(content=(f\"\"\"\n",
        "            Find job listings and industry trends for the following role in-demand skills:\n",
        "            {payChain.payload[\"role\"]}\n",
        "            \"\"\"\n",
        "            ))\n",
        "\n",
        "        ]\n",
        "        response = llm(messages).content\n",
        "        cot = json.loads(response.replace(\"```json\", '').replace(\"`\",\"\"))\n",
        "\n",
        "        all_answer, doc_source = search_(cot, top_analysis=5, top_search=2)\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=(\"\"\"\n",
        "            You are a market research analyst. Your task is to read the given main question, its sub-questions, and the relevant search results.\n",
        "            Summarize the findings into a clear, concise, and structured market research insight.\n",
        "\n",
        "            Guidelines:\n",
        "            - Provide a high-level overview answering the main question.\n",
        "            - Organize the summary around the sub-questions.\n",
        "            - Highlight key trends, required skills, salary ranges, challenges, and opportunities if available.\n",
        "            - Do not just copy the snippets, synthesize them into insights.\n",
        "            - Keep the tone professional and objective.\n",
        "\n",
        "            Return the output as a structured summary in plain text paragraphs.\n",
        "            \"\"\"\n",
        "            )),\n",
        "            HumanMessage(content=(f'''Below are several responses related to the given question:\n",
        "            {\"\\n\\n\".join(\n",
        "                [\n",
        "                    f\"\"\"sub-question: {cot['queries'][i]}\\nrelevant information for sub question: {'\\n\\n'.join(\n",
        "                        [\n",
        "                            f\"Title: {doc['title']}\\nLink: {doc['link']}\\nSnippet: {doc['snippet']}\" for doc in all_answer[i]\n",
        "                        ]\n",
        "                    )}\"\"\" for i in range(len(all_answer))\n",
        "                ]\n",
        "                )\n",
        "            }\n",
        "            '''\n",
        "            ))\n",
        "        ]\n",
        "\n",
        "\n",
        "        response = llm(messages).content\n",
        "        print(f\"Market Demands: {response}\")\n",
        "        # payChain.payload[\"market_demands\"] = json.loads(response.replace(\"```json\", '').replace(\"`\",\"\"))\n",
        "        payChain.payload[\"market_demands\"] = response\n",
        "        return {\n",
        "            payChain.payload[\"market_demands\"][:300]\n",
        "        }\n",
        "    return market_searcher\n",
        "\n",
        "def build_recomendation_and_report(payChain):\n",
        "    \"\"\"\n",
        "    Builds a LangChain tool for generating recommendations and reports based on analyzed data.\n",
        "    \"\"\"\n",
        "    @tool(\"recomendation_and_report\", return_direct=True)\n",
        "    def recomendation_and_report(tool_input: str):\n",
        "        \"\"\"\n",
        "        Synthesize the analysis from the two previous tools (CV parsing and skill analysis)\n",
        "        along with market search results to generate a comprehensive report. The report must include\n",
        "        a skill-gap analysis, identify key strengths, and propose a personalized upskilling plan\n",
        "        for the candidate.\n",
        "\n",
        "        Args:\n",
        "            tool_input (str): A JSON string or formatted text containing the combined results\n",
        "                              from the cv_parser, skill_analyst, and market_searcher tools.\n",
        "                              This input should ideally structure the data clearly (e.g.,\n",
        "                              {\"cv_data\": {...}, \"inferred_skills\": \"...\", \"market_trends\": \"...\"}).\n",
        "\n",
        "        Returns:\n",
        "            str: A formatted string representing the comprehensive report, including skill-gap\n",
        "                 analysis, strengths, and upskilling plan.\n",
        "        \"\"\"\n",
        "        messages = [\n",
        "            SystemMessage(content=(f\"\"\"\n",
        "            You are a career advisor and report generator. Your task is to synthesize the\n",
        "            provided data, which includes structured CV information, inferred skills, and\n",
        "            market trends for a specific role.\n",
        "\n",
        "            Based on this data, generate a comprehensive report that includes:\n",
        "            1.  **Skill-Gap Analysis**: Compare the candidate's skills (explicit and inferred)\n",
        "                with the in-demand skills identified from market research for the target role.\n",
        "                Clearly state the gaps.\n",
        "            2.  **Key Strengths**: Highlight the candidate's most relevant skills and experiences\n",
        "                that align with the target role requirements.\n",
        "            3.  **Personalized Upskilling Plan**: Propose actionable steps the candidate can take\n",
        "                to close the identified skill gaps. This could include suggestions for courses,\n",
        "                certifications, projects, or areas of study.\n",
        "\n",
        "            Present the report in a clear, well-structured, and easy-to-read format.\n",
        "            \"\"\"\n",
        "            )),\n",
        "            HumanMessage(content=(f\"\"\"\n",
        "            Generate a comprehensive career report based on the following analysis:\n",
        "\n",
        "            SKILL ANALYSIS:\n",
        "            {payChain.payload[\"detailed_analysis\"]}\n",
        "\n",
        "            MARKET DEMANDS SEARCH RESULT:\n",
        "            {payChain.payload[\"market_demands\"]}\n",
        "            \"\"\"\n",
        "            ))\n",
        "        ]\n",
        "        response = llm(messages).content\n",
        "        payChain.payload[\"report_and_recomendation\"] = response\n",
        "\n",
        "        return response\n",
        "    return recomendation_and_report"
      ],
      "metadata": {
        "id": "pqmRhgg7oG6q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Function"
      ],
      "metadata": {
        "id": "tba-pc32jUpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selected_tools(payChain, tool_names):\n",
        "    \"\"\"Select tools by name from the list of ALL TOOLS.\"\"\"\n",
        "    ALL_TOOLS = [\n",
        "        build_cv_parser(payChain),\n",
        "        build_skill_analyst(payChain),\n",
        "        build_market_searcher(payChain),\n",
        "        build_recomendation_and_report(payChain),\n",
        "    ]\n",
        "\n",
        "    selected_tools = []\n",
        "    for tool in ALL_TOOLS:\n",
        "        tool_name = getattr(tool, \"name\", None)\n",
        "        if tool_name in tool_names:\n",
        "            selected_tools.append(tool)\n",
        "\n",
        "    print(f\"Selected tools: {selected_tools}\")\n",
        "    return selected_tools\n",
        "\n",
        "def search_(cot, top_analysis=3, top_search=3):\n",
        "    \"\"\"\n",
        "    Function to perform searches based on questions that have been broken down into sub-questions.\n",
        "    \"\"\"\n",
        "    queries = cot[\"queries\"][:top_analysis]\n",
        "    all_answer = [search_engine_(q, top_search) for q in queries]\n",
        "\n",
        "    # Get source links from search results\n",
        "    doc_source = [source['link'] for answers in all_answer for source in answers]\n",
        "    print(\"\\n\\n\\ndoc_source:\", doc_source)\n",
        "    return all_answer, doc_source\n",
        "\n",
        "def search_engine_(question, k):\n",
        "    \"\"\"\n",
        "    Function to perform searches using the DuckDuckGo with more detailed results.\n",
        "    \"\"\"\n",
        "    json_result = []\n",
        "    with DDGS() as ddgs:\n",
        "        result = ddgs.text(question, region=\"us-en\", safesearch=\"moderate\", max_results=10)\n",
        "        for r in result:\n",
        "            json_result.append({\n",
        "                    \"title\":r['title'],\n",
        "                    \"link\":r['href'],\n",
        "                    \"snippet\":r['body']\n",
        "                })\n",
        "\n",
        "    print(f\"\\n------ Search Tool ------\\nSub-Question: {question}\\nAnswer: {json_result}\")\n",
        "    return faiss_json(question, json_result, k=k)\n",
        "\n",
        "def faiss_json(question, search_results, k=3):\n",
        "    \"\"\"\n",
        "    Function to perform searches using FAISS on JSON search results.\n",
        "    \"\"\"\n",
        "    user_input = {\"user_question\": question}\n",
        "    # Load Sentence-BERT model to generate vector embeddings\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2') # all-MiniLM-L6-v2, paraphrase-MiniLM-L6-v2\n",
        "\n",
        "    # Extract titles and snippets from search results\n",
        "    titles_and_snippets = [item['title'] + \" \" + item['snippet'] for item in search_results]\n",
        "\n",
        "    # Convert text to vectors using SentenceTransformer\n",
        "    context_vectors = model.encode(titles_and_snippets)\n",
        "\n",
        "    # Convert user question to a vector\n",
        "    user_question_vector = model.encode([user_input['user_question']])\n",
        "\n",
        "    # Create a FAISS index for context vectors\n",
        "    dimension = context_vectors.shape[1]  # Vector dimension (e.g., 384)\n",
        "    index = faiss.IndexFlatL2(dimension)  # Use L2 distance for searching\n",
        "    index.add(np.array(context_vectors, dtype=np.float32))\n",
        "\n",
        "    # Search: find the most relevant contexts for the user question\n",
        "    distances, indices = index.search(np.array(user_question_vector, dtype=np.float32), k)\n",
        "\n",
        "    # Collect results in JSON format\n",
        "    top_k_results = []\n",
        "    for i in range(k):\n",
        "        idx = indices[0][i]\n",
        "        result = {\n",
        "            \"title\": search_results[idx]['title'],\n",
        "            \"link\": search_results[idx]['link'],\n",
        "            \"snippet\": search_results[idx]['snippet'],\n",
        "            \"distance\": float(distances[0][i])  # Convert to float as JSON does not support np.float32\n",
        "        }\n",
        "        top_k_results.append(result)\n",
        "\n",
        "    # Return results in JSON format\n",
        "    return json.loads(json.dumps(top_k_results, indent=4))"
      ],
      "metadata": {
        "id": "VCwD5aciqmjh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Agent"
      ],
      "metadata": {
        "id": "AD04R5XfjY5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PayloadsChain():\n",
        "    \"\"\"\n",
        "    A simple class to manage a mutable payload dictionary that can be passed between components\n",
        "    or tools in a chain.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the PayloadsChain with an empty payload dictionary.\n",
        "        \"\"\"\n",
        "        self._payload = {} # Initialize an empty dictionary to store the payload\n",
        "\n",
        "    @property\n",
        "    def payload(self):\n",
        "        \"\"\"\n",
        "        Gets the current payload dictionary.\n",
        "        \"\"\"\n",
        "        return self._payload # Return the current payload\n",
        "\n",
        "    @payload.setter\n",
        "    def payload(self, value):\n",
        "        \"\"\"\n",
        "        Sets the payload dictionary.\n",
        "\n",
        "        Args:\n",
        "            value (dict): The dictionary to set as the payload.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If the provided value is not a dictionary.\n",
        "        \"\"\"\n",
        "        if not isinstance(value, dict):\n",
        "            raise TypeError('payload must be a dict') # Ensure the payload is a dictionary\n",
        "        self._payload = value # Set the internal payload to the provided value\n",
        "\n",
        "class ToolsBuilder():\n",
        "    \"\"\"\n",
        "    A class responsible for building a LangChain agent and selecting the tools it will use.\n",
        "    \"\"\"\n",
        "    def builder(self, llm, tool_names=[], payload={}):\n",
        "        \"\"\"\n",
        "        Builds a LangChain ReAct agent with the specified language model and tools.\n",
        "\n",
        "        Args:\n",
        "            llm: The language model instance to use for the agent.\n",
        "            tool_names (list[str], optional): A list of tool names to select. Defaults to [].\n",
        "            payload (dict, optional): An initial payload dictionary to pass to the tools. Defaults to {}.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the created LangChain agent and the list of selected tools.\n",
        "        \"\"\"\n",
        "        # Select the tools based on the provided names, passing the payload\n",
        "        tools = selected_tools(payChain, tool_names) # Select the tools from the available ones\n",
        "\n",
        "        # Define the template for the ReAct agent's prompt\n",
        "        REACT_TEMPLATE = \"\"\"You are Recruiter Assistent, a smart and helpful assistant.\n",
        "To solve the user’s request, you will explicitly follow a reasoning process using the format below:\n",
        "- Thought: Analyze the problem and consider the necessary steps\n",
        "- Action: Choose an action/tool to use (must be one of the available tools)\n",
        "- Action Input: The input for the selected action\n",
        "- Observation: The result of the action\n",
        "- ... (repeat Thought/Action/Action Input/Observation as needed)\n",
        "- Thought: I now have all the necessary information\n",
        "- Final Answer: The final response to the user\n",
        "\n",
        "AVAILABLE TOOLS:\n",
        "{tools}\n",
        "\n",
        "TOOL NAMES:\n",
        "{tool_names}\n",
        "\n",
        "CONVERSATION HISTORY:\n",
        "{chat_history}\n",
        "\n",
        "USER REQUEST:\n",
        "{input}\n",
        "\n",
        "REASONING PROCESS:\n",
        "{agent_scratchpad}\n",
        "\n",
        "Execution Guidelines:\n",
        "1. Use the tools only when relevant to the user’s request.\n",
        "2. Tools are designed to work sequentially:\n",
        "   - First, parse the CV with `cv_parser`\n",
        "   - Then analyze skills with `skill_analyst`\n",
        "   - Next, search the market with `market_searcher`\n",
        "   - Finally, generate recommendations and reports with `recomendation_and_report`\n",
        "3. Don't\n",
        "\n",
        "Always begin with \"Thought:\" when analyzing the request.\n",
        "\"\"\"\n",
        "\n",
        "        # Create a PromptTemplate from the defined template\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"tools\", \"tool_names\", \"chat_history\", \"input\", \"agent_scratchpad\"], # Define the input variables for the template\n",
        "            template=REACT_TEMPLATE # Use the defined template string\n",
        "        )\n",
        "\n",
        "        # Create a ReAct agent\n",
        "        agent = create_react_agent(\n",
        "            llm=llm, # Provide the language model\n",
        "            tools=tools, # Provide the selected tools\n",
        "            prompt=prompt, # Provide the prompt template\n",
        "        )\n",
        "        print(\"Agent has been created\") # Print a confirmation message\n",
        "        return agent, tools # Return the created agent and the selected tools"
      ],
      "metadata": {
        "id": "GvQ4ON1vsGQJ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CV"
      ],
      "metadata": {
        "id": "rlkUTxTZjbaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import (PyPDFLoader, UnstructuredPDFLoader, TextLoader, DirectoryLoader)\n",
        "\n",
        "def load_txt(path: str):\n",
        "  \"\"\"Simple example using TextLoader for .txt files\"\"\"\n",
        "  loader = TextLoader(path, encoding=\"utf-8\")\n",
        "  docs = loader.load()\n",
        "  return docs\n",
        "\n",
        "def load_pdf_pypdf(path: str):\n",
        "  \"\"\"Load PDF using PyPDFLoader (good for simple text PDFs).\"\"\"\n",
        "  loader = PyPDFLoader(path)\n",
        "  docs = loader.load()\n",
        "  return docs\n",
        "\n",
        "docs = load_pdf_pypdf(\"/content/CV Dastin Aryo Atmanto-EN-v2.pdf\")\n",
        "\n",
        "cv = \"\\n\".join([f\"{docs[i].page_content}\" for i in range(len(docs))])"
      ],
      "metadata": {
        "id": "DzYN6veKtChi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payChain = PayloadsChain()\n",
        "payChain.payload = {\n",
        "    \"text\": \"Please analyze this CV and create a comprehensive career report, including skills analysis, market opportunities, and recommendations\",\n",
        "    \"role\": \"Senior AI Engineer requirements\",\n",
        "    \"tools\": [\"cv_parser\", \"skill_analyst\", \"market_searcher\", \"recomendation_and_report\"],\n",
        "    \"curriculum_vitae\": cv,\n",
        "}\n",
        "\n",
        "print(payChain.payload)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXdO9XyNJBk1",
        "outputId": "0c023d2a-5d5f-4735-e915-458271343021"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Please analyze this CV and create a comprehensive career report, including skills analysis, market opportunities, and recommendations', 'role': 'Senior AI Engineer requirements', 'tools': ['cv_parser', 'skill_analyst', 'market_searcher', 'recomendation_and_report'], 'curriculum_vitae': 'Dastin Aryo Atmanto \\nAI Engineer | Data Science | IoT Engineer \\n    Bandung, Indonesia |        (+62) 858-6497-8583 |       dastinaryo5@gmail.com \\n   LinkedIn | Google Scholar | HaKI/Patens | Github \\nSUMMARY \\n \\nData & AI Engineer with 3 years of experience delivering end -to-end Artificial Intelligence and data -driven solutions \\nacross multiple roles, including concurrent positions as Data & AI Engineer. Skilled in developing and deploying AI –\\nIoT integrations, predictive analytics, and production -ready systems using TensorFlow, PyTorch, Flask, FastAPI, \\nRetrieval-Augmented Generation (RAG), and Large Language Models (LLM). Experienced in database analysis and \\nmanagement, with proven ability to balance multiple projects and deliver impactful results in fast -paced \\nenvironments. \\n \\nEXPERIENCE\\n \\nPT. INFORMASI TEKNOLOGI INDONESIA (Jatis Mobile) – Jakarta Selatan, Indonesia \\nAI Engineer – Full-time (Sep 2024 – Current)  \\n● Built a document anal yzer workflow that extracts data from tables and graphs. Utilized Docling for table \\nextraction and YOLO for graph detection, enabling structured data extraction and visualization from \\nunstructured documents. \\n● Handled 7 Proof-of-Concept (POC) projects related to the product, overseeing planning, implementation, and \\nevaluation, with 2 successful projects KAI and PlanetBan progressing to full implementation. \\n● Improved the RAG system by implementing a Cross -Encoder Reranker, resulting in a 20 –30% increase in \\nretrieval performance. \\n● Compared various AI models, including Qwen and Mistral, to determine the most suitable on -premise \\ndeployment solution for project requirements. \\n \\nPT. JIMMY GROUP TECHNOLOGY (SOCA AI) – Bandung, Indonesia \\nData & AI Engineer | RnD Staff – Freelance (Aug 2024 – Current) \\n● Delivered AI -driven conversational assistants for major clients (HOKIBank, Telkom Indonesia) using LLM, \\nRAG, and FastAPI, integrating NLP pipelines to boost service efficiency. \\n● Developed an AI -powered meeting notetaker with STT, transcription, and summarization, improving \\nproductivity and documentation. \\n● Built LSTM-based forecasting models for ticketing and network traffic, achieving 92.86% directional accuracy \\nand <1.1% MAPE for operational planning. \\n● Designed SQL database analysis, schema mapping, and ETL pipelines to enhance text -to-SQL RAG systems, \\nimproving query accuracy by ~70%. \\nAI Engineer | RnD Staff – Intern (Feb 2023 – Nov 2024) & Full-time (Dec 2023 – Aug 2024) \\n● Engineered LLM agents within a RAG framework to support virtual assistant services, improving accuracy and \\nresponsiveness using LangChain and LlamaIndex. \\n● Developed a robust data pipeline to automate the collection, processing, and flow of datasets, enhancing \\nefficiency in processing LLM. \\n● Developed AI model for wheels vehicle quality control using YOLOv8, Slicing Aided Hyper Inference (SAHI), \\nand custom SAHI modifications. Data preprocessing involved circle detection and color -space conversion with \\nOpenCV, and employed a U2Net-pretrained deep learning model. \\n● Developed NLP-based AI solutions for voice cloning (TTS/STT),  video generation, and auto-dubbing, covering \\nend-to-end lifecycle from research to MLOps deployment. \\n \\nPT. ORBIT FUTURE ACADEMY – Jakarta Selatan, Indonesia \\nAI Developer – Intern (Agu 2022 – Dec 2022) 5 months \\n● Conduct a comparative study of the Random Forest, XGBoost, LGBM, CatBoost, K -NN, and Adaboost \\nclassification models to determine the factors that affect the Human Development Index.  \\n● Build a deep learning model using the BERT Transfer Learning method to perform sentiment analysis on film \\nopinions. \\n● Implementing Q-Learning algorithm for learning Obstacle Avoidance Robot based on desktop game.  \\n● Deploy AI models on web -based applications using Flask API as a service for better user experience through \\nsmarter applications.\\nEDUCATION \\n \\nUNIVERSITAS PENDIDIKAN INDONESIA (2020 - 2024) \\nBachelor of Computer Engineering – GPA/IPK 3,94 \\n● 3rd place in the Internet of Things Edition 2022 Competition (LITE 2022) with the category of IT KTI \\ncompetition at the national level (2022). \\n● 2nd place in PILMAPRES at UPI Campus in Cibiru (2023). \\n● 1st place in HIMA TEKKOM Chess (2021). \\n● Finalist in Sebelas Maret International IoT Challenge 2021 (2021). \\n \\nPROJECTS \\n● Writer Identification using Handwriting IAM Dataset Based on Deep Learning  \\n● Offline Handwriting Writer Identification using Depth-wise Separable Convolution with Siamese Network \\n● Design and Implementation of an Internet of Things -Based Electrical Energy Consumption Monitoring and \\nPrediction Tool using Deep Learning \\n● IoT-based Traditional Market Crowd Detection System Application Prototype  \\n● Prototype Tracking Device for Mountain Climbers Using LoRa Technology and IoT Connectivity  \\n \\nTECHNICAL SKILLS \\n● AI & Data Science: Large Language Models (LLM), Retrieval-Augmented Generation (RAG), SQL databases, \\nTensorFlow, PyTorch, Pandas, Scikit-Learn, Numpy \\n● IoT & Robotics: Internet of Things (IoT), Robotics, FreeRTOS, Microcontrollers  \\n● Programming Languages: Python, Java, C++, C, PHP, SQL \\n● Web & Mobile Development: Web Programming (FastAPI, Flask), Android Programming (Java)  \\n \\nORGANISATION \\n \\nHIMA TEKKOM (Himpunan Mahasiswa Teknik Komputer) – Bandung, Indonesia \\n● Head of Commission I HIMA TEKKOM (Feb 2022 - Jan 2023) – Led team in planning and \\nexecuting student organization programs, coordinating across multiple departments.  \\n● Vice Chairman of HIMA TEKKOM Legislative Parliament (2021 - 2022) – Oversaw legislative \\nactivities, provided full guidance for organizational programs, and represented the organization in \\ninternal and external forums.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Agent"
      ],
      "metadata": {
        "id": "ljS0VEYOjed5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain, tools = ToolsBuilder().builder(llm, payChain.payload[\"tools\"], payload=payChain.payload)\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent_chain,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    max_iterations=10,\n",
        "    handle_parsing_errors=True,\n",
        "    return_intermediate_steps=True,\n",
        ")\n",
        "\n",
        "def execute_agent(payChain):\n",
        "    print(\"Exec Agent...\")\n",
        "    result = agent_executor.stream({\"input\": payChain.payload['text'], \"chat_history\": []})\n",
        "\n",
        "    for r in result:\n",
        "        yield r\n",
        "\n",
        "# Call the function to execute the agent\n",
        "for response in execute_agent(payChain):\n",
        "    print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YwVXG33wsqr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(payChain.payload[\"report_and_recomendation\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "xr6ByIT0hdm2",
        "outputId": "80d28cd7-ded5-4277-a9a3-eca176cef963"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Career Report for Dastin Aryo Atmanto\n\n## 1. Skill-Gap Analysis\n\n### Comparison of Skills\n\n**Candidate Skills:**\n- **Technical Skills:**\n  - Large Language Models (LLM)\n  - Retrieval-Augmented Generation (RAG)\n  - Database Management (SQL)\n  - Machine Learning Frameworks (TensorFlow, PyTorch)\n  - Web Development Frameworks (FastAPI, Flask)\n  - Data Analysis Libraries (Pandas, Scikit-Learn, Numpy)\n  - Internet of Things (IoT)\n  - Robotics and Embedded Systems\n  - Programming Languages (Python, Java, C++, C, PHP)\n\n- **Soft Skills:**\n  - Project Management\n  - Problem-Solving\n  - Communication\n  - Collaboration\n\n- **Domain Knowledge:**\n  - Artificial Intelligence\n  - Data-Driven Solutions\n  - Natural Language Processing (NLP)\n\n**In-Demand Skills for Senior AI Engineer (2023):**\n- Proficiency in generative AI\n- Advanced machine learning algorithms\n- Data analysis and interpretation\n- Familiarity with AI frameworks\n- Strong problem-solving and teamwork skills\n\n### Identified Gaps\n1. **Generative AI Expertise**: While Dastin has experience with LLMs, there is no explicit mention of generative AI, which is a growing focus in the industry.\n2. **Advanced Machine Learning Algorithms**: The candidate's experience with machine learning frameworks is strong, but specific advanced algorithms (e.g., reinforcement learning, GANs) are not highlighted.\n3. **Team Leadership**: Although Dastin has project management skills, there is limited evidence of experience in leading teams or mentoring junior engineers, which is increasingly important for senior roles.\n4. **Industry-Specific Applications**: Experience in applying AI in specific industries (e.g., healthcare, finance) is not mentioned, which could be beneficial for certain roles.\n\n## 2. Key Strengths\n\n- **Strong Technical Foundation**: Dastin's proficiency in LLMs, RAG, and machine learning frameworks positions him well for AI engineering roles.\n- **Diverse Programming Skills**: Knowledge of multiple programming languages enhances his versatility in software development.\n- **Project Management Experience**: Successfully managing multiple POC projects demonstrates his ability to oversee complex initiatives and deliver results.\n- **Effective Communication**: His ability to convey technical information to non-technical stakeholders is crucial for collaboration in cross-functional teams.\n- **Deep Domain Knowledge**: Extensive experience in AI and data-driven solutions provides a solid foundation for tackling complex engineering challenges.\n\n## 3. Personalized Upskilling Plan\n\nTo close the identified skill gaps and enhance Dastin's qualifications for a Senior AI Engineer role, the following actionable steps are recommended:\n\n### Courses and Certifications\n1. **Generative AI Specialization**: Enroll in a course focused on generative models (e.g., GANs, VAEs) to gain expertise in this emerging area.\n2. **Advanced Machine Learning**: Take advanced courses on machine learning algorithms, including reinforcement learning and deep learning techniques.\n3. **Leadership and Team Management**: Participate in workshops or courses on leadership skills, focusing on team dynamics and mentoring.\n\n### Projects\n1. **Industry-Specific AI Projects**: Engage in projects that apply AI solutions to specific industries (e.g., healthcare diagnostics, financial forecasting) to gain relevant experience.\n2. **Open Source Contributions**: Contribute to open-source AI projects to enhance coding skills and collaborate with other developers.\n\n### Networking and Professional Development\n1. **Join AI and Data Science Communities**: Participate in forums, meetups, and conferences to network with industry professionals and stay updated on trends.\n2. **Mentorship**: Seek mentorship from experienced AI engineers to gain insights into leadership and career progression in the field.\n\n### Continuous Learning\n1. **Stay Updated on Industry Trends**: Regularly read industry publications and research papers to keep abreast of the latest advancements in AI technologies and methodologies.\n\nBy following this personalized upskilling plan, Dastin Aryo Atmanto can effectively bridge the skill gaps identified and position himself as a strong candidate for Senior AI Engineer roles in the evolving tech landscape."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(payChain.payload[\"report_and_recomendation\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QthlVLbsuHcx",
        "outputId": "3f3ec392-99ec-4f98-8b0a-3be977a94999"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Career Report for Dastin Aryo Atmanto\n\n## 1. Skill-Gap Analysis\n\n### Comparison of Skills\n\n| **Skill Category**                | **Dastin's Skills**                                                                 | **In-Demand Skills**                                                                 | **Gap**                                                                 |\n|-----------------------------------|-------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n| **Programming Languages**          | Python                                                                              | Python, Java, R                                                                      | Java and R are not mentioned in Dastin's skills.                       |\n| **Machine Learning Frameworks**    | TensorFlow, PyTorch                                                                | TensorFlow, PyTorch, Keras                                                          | Keras is not mentioned.                                                  |\n| **Data Handling and Processing**   | SQL, ETL                                                                           | SQL, NoSQL (MongoDB, Cassandra), Big Data Technologies (Apache Spark, Hadoop)      | NoSQL databases and Big Data technologies are not mentioned.            |\n| **AI and Machine Learning Concepts**| Deep Learning, NLP (BERT)                                                          | Deep Learning (CNNs, RNNs, GANs), NLP, Computer Vision                             | Specific knowledge in CNNs, RNNs, GANs, and Computer Vision is lacking.|\n| **Cloud Computing**                | Not mentioned                                                                       | AWS, Azure, Google Cloud, Containerization (Docker, Kubernetes)                    | No experience in cloud computing or containerization.                   |\n| **Software Development Practices**  | Not mentioned                                                                       | Version Control (Git), Agile Methodologies                                          | No experience in version control or Agile practices.                    |\n| **Mathematics and Statistics**     | Strong foundation inferred                                                          | Strong foundation in linear algebra, calculus, and probability                      | Specific mention of advanced mathematics is lacking.                    |\n| **Soft Skills**                    | Problem-Solving, Collaboration, Communication                                      | Problem-Solving, Collaboration, Communication                                       | No gaps identified in soft skills.                                      |\n\n### Summary of Gaps\n- **Programming Languages**: Lack of experience in Java and R.\n- **Machine Learning Frameworks**: No experience with Keras.\n- **Data Handling**: No knowledge of NoSQL databases or Big Data technologies.\n- **AI Concepts**: Lacking specific knowledge in CNNs, RNNs, RNNs, GANs, and Computer Vision.\n- **Cloud Computing**: No experience with cloud platforms or containerization.\n- **Software Development Practices**: No experience with version control or Agile methodologies.\n- **Mathematics**: Lack of specific mention of advanced mathematics.\n\n---\n\n## 2. Key Strengths\n\n### Relevant Skills and Experiences\n- **Technical Proficiency**: Strong skills in AI and ML, particularly with TensorFlow and PyTorch, which are highly relevant for the Senior AI Engineer role.\n- **Database Management**: Proficient in SQL and ETL processes, indicating a solid foundation in data handling.\n- **Web Development**: Experience with Flask and FastAPI, showcasing the ability to build applications that can integrate AI models.\n- **Project Management**: Proven ability to manage multiple projects, indicating strong organizational skills.\n- **Leadership**: Experience in leadership roles, demonstrating the ability to lead teams and coordinate efforts effectively.\n- **Communication Skills**: Strong verbal and written communication skills, essential for conveying complex technical concepts to diverse audiences.\n\n---\n\n## 3. Personalized Upskilling Plan\n\n### Actionable Steps to Close Skill Gaps\n\n1. **Programming Languages**:\n   - **Courses**: Enroll in online courses for Java and R (e.g., Coursera, Udemy).\n   - **Projects**: Implement small projects using Java and R to solidify understanding.\n\n2. **Machine Learning Frameworks**:\n   - **Courses**: Take a course on Keras to learn about rapid prototyping of deep learning models.\n   - **Projects**: Create a project that utilizes Keras for a specific AI application.\n\n3. **Data Handling**:\n   - **Courses**: Learn about NoSQL databases (MongoDB, Cassandra) and Big Data technologies (Apache Spark, Hadoop).\n   - **Projects**: Work on a project that involves data processing using these technologies.\n\n4. **AI Concepts**:\n   - **Courses**: Study advanced AI concepts, including CNNs, RNNs, GANs, and Computer Vision.\n   - **Projects**: Develop a project that incorporates these concepts, such as an image classification model.\n\n5. **Cloud Computing**:\n   - **Courses**: Take courses on AWS, Azure, or Google Cloud, focusing on deploying AI models.\n   - **Projects**: Deploy an existing AI model on a cloud platform to gain practical experience.\n\n6. **Software Development Practices**:\n   - **Courses**: Learn about Git for version control and Agile methodologies.\n   - **Projects**: Collaborate on open-source projects to practice version control and Agile practices.\n\n7. **Mathematics and Statistics**:\n   - **Courses**: Take advanced mathematics courses focusing on linear algebra, calculus, and probability.\n   - **Study**: Engage in self-study or join study groups to reinforce mathematical concepts relevant to AI.\n\n### Conclusion\nDastin Aryo Atmanto has a strong foundation in AI and machine learning, complemented by relevant technical and soft skills. By addressing the identified skill gaps through targeted courses and projects, he can enhance his qualifications for a Senior AI Engineer role and position himself for success in the evolving AI landscape. Continuous learning and practical application will be key to his professional growth."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"report.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(payChain.payload[\"report_and_recomendation\"])\n"
      ],
      "metadata": {
        "id": "s3a5ayyTenZa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0x_M7EAmXur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}